diff --git a/contrib/jemalloc/include/jemalloc/jemalloc_FreeBSD.h b/contrib/jemalloc/include/jemalloc/jemalloc_FreeBSD.h
index 7eeab54696e..a19cbb114ef 100644
--- a/contrib/jemalloc/include/jemalloc/jemalloc_FreeBSD.h
+++ b/contrib/jemalloc/include/jemalloc/jemalloc_FreeBSD.h
@@ -72,6 +72,10 @@
 #  define LG_SIZEOF_PTR		3
 #endif
 
+#if LG_VADDR > 32
+#  define JEMALLOC_RETAIN
+#endif
+
 #ifndef JEMALLOC_TLS_MODEL
 #  define JEMALLOC_TLS_MODEL	/* Default. */
 #endif
diff --git a/sys/amd64/Makefile b/sys/amd64/Makefile
index 2e87b95895e..641e7dc8dc6 100644
--- a/sys/amd64/Makefile
+++ b/sys/amd64/Makefile
@@ -14,7 +14,7 @@ all:
 	@echo "make links or tags only"
 
 # Directories in which to place amd64 tags links
-DAMD64=	acpica amd64 ia32 include linux linux32 pci vmm
+DAMD64=	acpica amd64 gmem ia32 include linux linux32 pci vmm
 
 links::
 	-for i in ${COMMDIR1}; do \
@@ -25,7 +25,9 @@ links::
 	    (cd $$i && { rm -f tags; ln -s ../tags tags; }) done
 
 SAMD64=	${SYS}/amd64/acpica/*.[ch] \
-	${SYS}/amd64/amd64/*.[ch] ${SYS}/amd64/ia32/*.[ch] \
+	${SYS}/amd64/amd64/*.[ch] \
+	${SYS}/amd64/gmem/*.[ch] \
+	${SYS}/amd64/ia32/*.[ch] \
 	${SYS}/amd64/include/*.[ch] ${SYS}/amd64/linux/*.[ch] \
 	${SYS}/amd64/linux32/*.[ch] ${SYS}/amd64/pci/*.[ch] \
 	${SYS}/amd64/vmm/*.[ch]
diff --git a/sys/amd64/conf/GENERIC b/sys/amd64/conf/GENERIC
index 1851ff63585..a0cb16f47ae 100644
--- a/sys/amd64/conf/GENERIC
+++ b/sys/amd64/conf/GENERIC
@@ -366,3 +366,6 @@ device		netmap			# netmap(4) support
 options 	EVDEV_SUPPORT		# evdev support in legacy drivers
 device		evdev			# input event device support
 device		uinput			# install /dev/uinput cdev
+
+# GMEM interface
+options 	GMEM
diff --git a/sys/amd64/gmem/gmem_dev.c b/sys/amd64/gmem/gmem_dev.c
new file mode 100644
index 00000000000..52166861dbe
--- /dev/null
+++ b/sys/amd64/gmem/gmem_dev.c
@@ -0,0 +1,142 @@
+/*-
+ * 
+ * This software was developed by Weixi Zhu <wxzhu@rice.edu>
+ * for gmem project.
+ *
+ * GMEM is a generic memory management interface for CPU and devices
+ * It currently only supports x86_64 platforms.
+ *
+ */
+
+#include <sys/cdefs.h>
+#include <sys/param.h>
+#include <sys/domainset.h>
+#include <sys/kernel.h>
+#include <sys/lock.h>
+#include <sys/proc.h>
+#include <sys/rwlock.h>
+#include <sys/malloc.h>
+#include <sys/sysctl.h>
+#include <sys/systm.h>
+#include <sys/selinfo.h>
+#include <sys/smp.h>
+#include <sys/pipe.h>
+#include <sys/bio.h>
+#include <sys/buf.h>
+#include <sys/bus.h>
+#include <sys/vmem.h>
+#include <sys/vmmeter.h>
+#include <sys/tree.h>
+
+#include <vm/vm.h>
+#include <vm/vm_object.h>
+#include <vm/vm_page.h>
+
+#include <sys/gmem.h>
+#include <amd64/gmem/gmem_dev.h>
+#include <amd64/gmem/gmem_uvas.h>
+
+static gmem_devmap_t gmem_devmap_store;
+#define gmem_devmap (&gmem_devmap_store)
+
+static SYSCTL_NODE(_vm, OID_AUTO, gmem, CTLFLAG_RD, 0, "GMEM Info");
+
+static int gmem_inited = 0;
+SYSCTL_INT(_vm_gmem, OID_AUTO, gmem_init, CTLFLAG_RD,
+    &gmem_inited, 0,
+    "Whether gmem is initialized");
+
+static void gmem_dev_init(void);
+SYSINIT(gmem_dev_init, SI_SUB_DRIVERS - 1, SI_ORDER_FIRST, gmem_dev_init, NULL);
+
+static void gmem_dev_init(void)
+{
+	int i;
+
+	mtx_init(&gmem_devmap->lock, "global gmem device map", NULL, MTX_DEF);
+	gmem_devmap->unr = new_unrhdr(0, MAXNGMEMDEV - 1, NULL);
+	for (i = 0; i < MAXNGMEMDEV; i ++)
+		gmem_devmap->dev[i] = NULL;
+
+	gmem_inited = 1;
+	printf("GMEM DEV INIT PASS\n");
+}
+
+// allocate a device in gmem_devmap
+static gmem_dev_t* gmem_devmap_alloc_dev()
+{
+	uint16_t dev_id;
+
+	GMEM_DEVMAP_ASSERT_LOCKED(gmem_devmap);
+
+	dev_id = alloc_unr(gmem_devmap->unr);
+	if (dev_id + 1 == 0)
+		panic("gmem dev id allocation failed");
+
+	gmem_devmap->dev[dev_id] = malloc(sizeof(gmem_dev_t), M_DEVBUF, M_WAITOK);
+	if (gmem_devmap->dev[dev_id] == NULL)
+		panic("ENOMEM allocating new gmem dev struct");
+	gmem_devmap->dev[dev_id]->id = dev_id;
+
+	return gmem_devmap->dev[dev_id];
+}
+
+// free a device in gmem_devmap
+static void gmem_devmap_free_dev(gmem_dev_t *gmem_dev)
+{
+	uint16_t dev_id;
+
+	GMEM_DEVMAP_ASSERT_LOCKED(gmem_devmap);
+
+	// recycle dev_id
+	dev_id = gmem_dev->id;
+	free_unr(gmem_devmap->unr, dev_id);
+
+	// clear references
+	device_set_gmem_dev(gmem_dev->device, NULL);
+	gmem_devmap->dev[dev_id] = NULL;
+
+	free(gmem_dev, M_DEVBUF);
+}
+
+static gmem_dev_t * gmem_devmap_lookup_id(uint16_t dev_id)
+{
+	GMEM_DEVMAP_ASSERT_LOCKED(gmem_devmap);
+
+	return gmem_devmap->dev[dev_id];
+}
+
+gmem_dev_t * gmem_dev_add(device_t device, gmem_mmu_ops_t *mmu_ops)
+{
+	gmem_dev_t* dev;
+
+	dev = device_get_gmem_dev(device);
+	if (dev != NULL && dev == gmem_devmap_lookup_id(dev->id))
+	{
+		// TODO: add a new sysctl int to count redundantly added devices
+		printf("REDUNDANT GMEM DEV ADD DETECTED");
+		return dev;
+	}
+
+	GMEM_DEVMAP_LOCK(gmem_devmap);
+	dev = gmem_devmap_alloc_dev();
+	device_set_gmem_dev(device, dev);
+	dev->device = device;
+	dev->cur_pmap = NULL;
+	dev->mmu_ops = mmu_ops;
+	GMEM_DEVMAP_UNLOCK(gmem_devmap);
+
+	return dev;
+}
+
+void gmem_dev_remove(gmem_dev_t *dev)
+{
+	GMEM_DEVMAP_LOCK(gmem_devmap);
+	gmem_devmap_free_dev(dev);
+	GMEM_DEVMAP_UNLOCK(gmem_devmap);
+}
+
+bool is_gmem_dev(device_t device)
+{
+	return (device_get_gmem_dev(device) != NULL);
+}
\ No newline at end of file
diff --git a/sys/amd64/gmem/gmem_dev.h b/sys/amd64/gmem/gmem_dev.h
new file mode 100644
index 00000000000..1573264f6f1
--- /dev/null
+++ b/sys/amd64/gmem/gmem_dev.h
@@ -0,0 +1,59 @@
+/*-
+ * 
+ * This software was developed by Weixi Zhu <wxzhu@rice.edu>
+ * for gmem project.
+ *
+ * GMEM is a generic memory management interface for CPU and devices
+ * It currently only supports x86_64 platforms.
+ *
+ */
+
+#ifndef _GMEM_DEV_H_
+#define	_GMEM_DEV_H_
+
+#define MAXNGMEMDEV 256
+
+// Right now we assume SR-IOV as the only way to partition a device. 
+// No context switch should happen for time-based division.
+// As a start GMEM only considers unified address space. 
+struct gmem_dev
+{
+	// a pointer to its FreeBSD's struct device
+	device_t device;
+
+	// A unique identifier for gmem_dev
+	uint16_t id;
+
+	// cached pmap pointer, updated by device context switch
+	dev_pmap_t *cur_pmap;
+
+	// Do not consider:
+	//   1. hiererchical MMUs for nested translations, as gmem interfaces should run independently
+	// To consider:
+	//   1. dynamic mmu changes. E.g. device hotplug in guest OS.
+	gmem_mmu_ops_t *mmu_ops;
+
+	// list of gmem devs sharing the same pmap
+	TAILQ_ENTRY(gmem_dev) gmem_dev_list;
+};
+
+// global list (hashmap) of gmem devices 
+struct gmem_devmap_info
+{
+	struct mtx lock;
+	struct unrhdr *unr;
+	gmem_dev_t* dev[MAXNGMEMDEV];
+};
+
+// GMEM KPIs for devices
+gmem_dev_t* gmem_dev_add(device_t device, gmem_mmu_ops_t *mmu_ops);
+void gmem_dev_remove(gmem_dev_t *dev);
+bool is_gmem_dev(device_t device);
+
+#define	GMEM_DEVMAP_ASSERT_LOCKED(devmap)		mtx_assert(&(devmap)->lock, MA_OWNED)
+#define	GMEM_DEVMAP_ASSERT_UNLOCKED(devmap)		mtx_assert(&(devmap)->lock, MA_NOTOWNED)
+
+#define	GMEM_DEVMAP_LOCK(devmap)				mtx_lock(&(devmap)->lock)
+#define	GMEM_DEVMAP_UNLOCK(devmap)				mtx_unlock(&(devmap)->lock)
+
+#endif
\ No newline at end of file
diff --git a/sys/amd64/gmem/gmem_uvas.c b/sys/amd64/gmem/gmem_uvas.c
new file mode 100644
index 00000000000..f00ed0ee367
--- /dev/null
+++ b/sys/amd64/gmem/gmem_uvas.c
@@ -0,0 +1,202 @@
+/*-
+ * 
+ * This software was developed by Weixi Zhu <wxzhu@rice.edu>
+ * for gmem project.
+ *
+ * GMEM is a generic memory management interface for CPU and devices
+ * It currently only supports x86_64 platforms.
+ *
+ */
+
+#include <sys/cdefs.h>
+#include <sys/param.h>
+#include <sys/domainset.h>
+#include <sys/kernel.h>
+#include <sys/lock.h>
+#include <sys/proc.h>
+#include <sys/rwlock.h>
+#include <sys/malloc.h>
+#include <sys/sysctl.h>
+#include <sys/systm.h>
+#include <sys/selinfo.h>
+#include <sys/smp.h>
+#include <sys/pipe.h>
+#include <sys/bio.h>
+#include <sys/buf.h>
+#include <sys/vmem.h>
+#include <sys/vmmeter.h>
+#include <sys/tree.h>
+
+#include <vm/vm.h>
+#include <vm/vm_object.h>
+#include <vm/vm_page.h>
+
+#include <sys/gmem.h>
+#include <amd64/gmem/gmem_dev.h>
+#include <amd64/gmem/gmem_uvas.h>
+
+static gmem_error_t gmem_uvas_alloc_span(gmem_uvas_t *uvas, vm_offset_t *start, 
+	vm_size_t size, vm_prot_t protection, dev_pmap_t *pmap, 
+	gmem_uvas_entry_t *entry);
+static gmem_error_t gmem_uvas_free_span(gmem_uvas_t *uvas, vm_offset_t start,
+	vm_size_t size);
+
+static int gmem_uvas_cmp_entries(struct gmem_uvas_entry *a, struct gmem_uvas_entry *b)
+{
+	// copied from iommu code
+	KASSERT(a->start <= a->end, ("inverted entry %p (%jx, %jx)",
+	    a, (uintmax_t)a->start, (uintmax_t)a->end));
+	KASSERT(b->start <= b->end, ("inverted entry %p (%jx, %jx)",
+	    b, (uintmax_t)b->start, (uintmax_t)b->end));
+	KASSERT(a->end <= b->start || b->end <= a->start ||
+	    a->end == a->start || b->end == b->start,
+	    ("overlapping entries %p (%jx, %jx) %p (%jx, %jx)",
+	    a, (uintmax_t)a->start, (uintmax_t)a->end,
+	    b, (uintmax_t)b->start, (uintmax_t)b->end));
+
+	if (a->end < b->end)
+		return (-1);
+	else if (b->end < a->end)
+		return (1);
+	return (0);
+}
+
+RB_GENERATE(gmem_uvas_entries_tree, gmem_uvas_entry, rb_entry,
+    gmem_uvas_cmp_entries);
+
+// Three modes to use uvas:
+// 	1. private: pmap is NULL && replicate == false
+//  2. shared: uvas and pmap are both not NULL, replicate == false
+//  3. replicate: uvas and pmap are both not NULL, replicate == true
+gmem_error_t gmem_uvas_create(gmem_uvas_t *uvas, vm_size_t size, gmem_dev_t *dev,
+	dev_pmap_t *pmap, void *dev_data, bool replicate, bool need_partial_update)
+{
+	if (uvas == NULL)
+	{
+		KASSERT(pmap == NULL, "Creating a uvas with non-null pmap");
+		KASSERT(data == NULL, "Creating a uvas with non-null dev-specific data");
+
+		// allocate and create the pmap with dev->mmu_ops
+		pmap = malloc(sizeof(dev_pmap_t), M_DEVBUF, M_WAITOK | M_ZERO);
+		// allocate and create the uvas
+		uvas = malloc(sizeof(gmem_uvas_t), M_DEVBUF, M_WAITOK | M_ZERO);
+
+		// initialize pmap
+		pmap->ndevices = 1;
+		TAILQ_INIT(pmap->gmem_dev_header);
+		TAILQ_INSERT_TAIL(&pmap->gmem_dev_header, dev, gmem_dev_list);
+		pmap->mmu_ops = dev->mmu_ops;
+		pmap->pmap_replica = NULL;
+		pmap->uvas = uvas;
+
+		// use mmu callback to initialize device-specific data
+		pmap->mmu_ops->mmu_pmap_create(&pmap->data, dev_data);
+
+		// initialize uvas
+		TAILQ_INIT(uvas->uvas_entry_header);
+		TAILQ_INIT(uvas->dev_pmap_header);
+		TAILQ_INSERT_TAIL(&uvas->dev_pmap_header, pmap, unified_pmap_list);
+		uvas->need_partial_update = need_partial_update
+		uvas->size = size;
+		if (need_partial_update)
+		{
+			// Currently we use no quantum cache
+			uvas->arena = vmem_create("uva", 0, rounddown(size, PAGE_SIZE),
+				PAGE_SIZE, 0, M_WAITOK);
+		}
+		else
+		{
+			// TODO: RB-TREE
+			// RB_INIT(uvas->rb_root);
+		}
+	}
+	else
+	{
+		// attach dev and pmap to the uvas
+		panic("Attaching to a uvas is not implemented");
+	}
+	return GMEM_OK;
+}
+
+gmem_error_t gmem_uvas_delete(gmem_uvas_t *uvas)
+{
+	KASSERT(uvas != NULL, "The uvas to be deleted is NULL!");
+
+	// traverse all pmaps of the uvas and delete them
+
+	// free the uvas
+	return GMEM_OK;
+}
+
+static gmem_error_t gmem_uvas_alloc_span(gmem_uvas_t *uvas, vm_offset_t *start, 
+	vm_size_t size, vm_prot_t protection, dev_pmap_t *pmap, 
+	gmem_uvas_entry_t *entry)
+{
+	KASSERT(uvas != NULL, "The uvas to allocate entry is NULL!");
+	if (uvas->need_partial_update)
+	{
+		// use rb-tree allocator
+	}
+	else
+	{
+		// use vmem allocator
+	}
+	return GMEM_OK;
+}
+
+static gmem_error_t gmem_uvas_free_span(gmem_uvas_t *uvas, vm_offset_t start,
+	vm_size_t size)
+{
+	KASSERT(uvas != NULL, "The uvas to allocate entry is NULL!");
+	if (uvas->need_partial_update)
+	{
+		// use rb-tree allocator
+	}
+	else
+	{
+		// use vmem allocator
+	}
+	return GMEM_OK;
+}
+
+gmem_error_t gmem_uvas_map_pages(dev_pmap_t *pmap, vm_offset_t start,
+	vm_size_t size, vm_page_t first_page)
+{
+	KASSERT(pmap != NULL, "The pmap to map is NULL!");
+
+	return GMEM_OK;
+}
+
+gmem_error_t gmem_uvas_map_pages_sg(dev_pmap_t *pmap, vm_offset_t start,
+	vm_size_t size, vm_page_t *pages)
+{
+	KASSERT(pmap != NULL, "The pmap to map is NULL!");
+
+	return GMEM_OK;
+}
+
+gmem_error_t gmem_uvas_unmap(dev_pmap_t *pmap, vm_offset_t start,
+	vm_size_t size, void (* unmap_callback(void *)), void *callback_args)
+{
+	KASSERT(pmap != NULL, "The pmap to unmap is NULL!");
+
+	// Think about how to async?
+	if (unmap_callback == NULL)
+	{
+		// The unmap will be sync
+	}
+	else
+	{
+		// The unmap will be async
+	}
+
+	return GMEM_OK;
+}
+
+gmem_error_t gmem_uvas_protect(gmem_uvas_t *uvas, vm_offset_t start,
+	vm_size_t size, vm_prot_t new_protection)
+{
+	KASSERT(uvas != NULL, "The uvas to mutate protection is NULL!");
+
+	return GMEM_OK;
+}
\ No newline at end of file
diff --git a/sys/amd64/gmem/gmem_uvas.h b/sys/amd64/gmem/gmem_uvas.h
new file mode 100644
index 00000000000..c94a26097d6
--- /dev/null
+++ b/sys/amd64/gmem/gmem_uvas.h
@@ -0,0 +1,140 @@
+/*-
+ * 
+ * This software was developed by Weixi Zhu <wxzhu@rice.edu>
+ * for gmem project.
+ *
+ * GMEM is a generic memory management interface for CPU and devices
+ * It currently only supports x86_64 platforms.
+ *
+ */
+
+#ifndef _GMEM_UVAS_H_
+#define	_GMEM_UVAS_H_
+
+struct gmem_mmu_ops
+{
+	// bitmap of available page shifts for page-based TLB
+	unsigned long pgsize_bitmap;
+	bool mmu_has_range_tlb;
+
+	// device page/range table creation and destruction.
+	gmem_error_t (*mmu_pmap_create)(dev_pmap_t *pmap, void *dev_data);
+	gmem_error_t (*mmu_pmap_destroy)(dev_pmap_t *pmap);
+
+	// device mapping creation, manipulation and destruction
+	// We do not include batching mechanism here, as we will not exercise
+	// any throughput devices at this moment
+	// We also do not include async version for map, as it will not be used
+	gmem_error_t (*mmu_pmap_enter)(vm_offset_t va, vm_size_t size, 
+		vm_paddr_t pa, vm_prot_t protection);
+	gmem_error_t (*mmu_pmap_release)(vm_offset_t va, vm_size_t size);
+	gmem_error_t (*mmu_pmap_protect)(vm_offset_t va, vm_size_t size,
+		vm_prot_t new_prot);
+};
+
+RB_HEAD(gmem_uvas_entries_tree, gmem_uvas_entry);
+RB_PROTOTYPE(gmem_uvas_entries_tree, gmem_uvas_entry, rb_entry,
+    gmem_uvas_cmp_entries);
+
+struct gmem_uvas // VM counterpart: struct vm_map
+{
+	// List of mapped entries
+	TAILQ_HEAD(gmem_uvas_entry_tailq, gmem_uvas_entry) uvas_entry_header;
+
+	// Whether this uvas needs partial update of its entries
+	// 	This determines whether it uses vmem or rb-tree to allocate/free 
+	//  uvas entries.
+	bool need_partial_update;
+
+	// uva arena for va allocation
+	// We have to use a rb tree entry to support split/merge
+	// but we may use vmem as well to allocate va span quickly
+	vmem_t *arena;
+
+	// Number of entires
+	uint32_t nentries;
+
+	// virtual size
+	vm_size_t size;
+
+	// A uvas may be used by multiple pmaps (mmus)
+	TAILQ_HEAD(dev_pmap_tailq, dev_pmap) dev_pmap_header;
+
+	struct gmem_uvas_entries_tree rb_root;
+};
+
+// split and merge may be applied if protection or vm_ops change
+struct gmem_uvas_entry // VM counterpart: struct vm_map_entry
+{
+	vm_offset_t start;
+	vm_offset_t end;
+
+	vm_object_t object;	/* the vm object this entry point to */
+	vm_ooffset_t offset;		/* offset into object */
+
+	// May skip it if we use vmem to allocate/free va
+	vm_size_t max_free;
+
+	// a doubly linked list of entries sorted by base
+	TAILQ_ENTRY(gmem_uvas_entry) mapped_entry;
+
+	// rb-tree entry for query/delete/insert
+	RB_ENTRY(gmem_uvas_entry) rb_entry;
+
+	// changes of protection or pmap may result in splitting or merging
+	vm_prot_t protection;
+};
+
+// A collection of pmaps that are registed in replication mode for a uvas
+// devices of the pmap must come from the same NUMA group
+// Well, at this moment all pmap come from a single NUMA group.
+struct dev_pmap_replica
+{
+	uint8_t npmaps;
+	struct dev_pmap **replicated_pmaps;
+};
+
+// device-dependent mapping data
+// A pmap is coupled with an mmu instance
+struct dev_pmap
+{
+	// An array of the mapping devices
+	uint8_t ndevices;
+	// for convenience, use a tailq for devices sharing this dev_pmap
+	TAILQ_HEAD(gmem_dev_tailq, gmem_dev) gmem_dev_header;
+
+	struct dev_pmap_replica *pmap_replica;
+
+	// A pointer to its unified address space
+	struct gmem_uvas *uvas;
+
+	// list of pmaps included by a uvas
+	TAILQ_ENTRY(dev_pmap) unified_pmap_list;
+
+	// MMU ops to mutate the pmap. 
+	// TO Think:
+	// 1. Could it change dynamically?
+	// 2. What happens when multiple devices share the same type of mmu?
+	struct gmem_mmu_ops *mmu_ops;
+
+	// Device-specific page table data to be operated by gmem_mmu_ops
+	// can include a child_pmap for nested translation
+	// can also store the implementation of mmu_ops
+	void *data;
+};
+
+gmem_error_t gmem_uvas_create(gmem_uvas_t *uvas, vm_size_t size, gmem_dev_t *dev,
+	dev_pmap_t *pmap, void *dev_data, bool replicate, bool need_partial_update);
+gmem_error_t gmem_uvas_delete(gmem_uvas_t *uvas);
+gmem_error_t gmem_uvas_map_pages(dev_pmap_t *pmap, vm_offset_t start,
+	vm_size_t size, vm_page_t first_page);
+gmem_error_t gmem_uvas_map_pages_sg(dev_pmap_t *pmap, vm_offset_t start,
+	vm_size_t size, vm_page_t *pages);
+gmem_error_t gmem_uvas_unmap(dev_pmap_t *pmap, vm_offset_t start,
+	vm_size_t size, void (* unmap_callback(void *)),
+	void *callback_args);
+gmem_error_t gmem_uvas_protect(gmem_uvas_t *uvas, vm_offset_t start,
+	vm_size_t size, vm_prot_t new_protection);
+
+
+#endif
\ No newline at end of file
diff --git a/sys/conf/files.amd64 b/sys/conf/files.amd64
index feecdd467b7..f5e3bac840d 100644
--- a/sys/conf/files.amd64
+++ b/sys/conf/files.amd64
@@ -723,6 +723,7 @@ x86/iommu/intel_fault.c		optional	acpi acpi_dmar pci
 x86/iommu/intel_gas.c		optional	acpi acpi_dmar pci
 x86/iommu/intel_idpgtbl.c	optional	acpi acpi_dmar pci
 x86/iommu/intel_intrmap.c	optional	acpi acpi_dmar pci
+x86/iommu/intel_iommu.c		optional	acpi acpi_dmar pci
 x86/iommu/intel_qi.c		optional	acpi acpi_dmar pci
 x86/iommu/intel_quirks.c	optional	acpi acpi_dmar pci
 x86/iommu/intel_utils.c		optional	acpi acpi_dmar pci
@@ -771,3 +772,8 @@ x86/xen/xenpv.c			optional	xenhvm
 x86/xen/xen_nexus.c		optional	xenhvm
 x86/xen/xen_msi.c		optional	xenhvm
 x86/xen/xen_pci_bus.c		optional	xenhvm
+#
+# GMEM support
+#
+amd64/gmem/gmem_dev.c 	optional gmem
+amd64/gmem/gmem_uvas.c 	optional gmem
\ No newline at end of file
diff --git a/sys/conf/options b/sys/conf/options
index b5b8ad09bf5..555e18a4eb4 100644
--- a/sys/conf/options
+++ b/sys/conf/options
@@ -1025,3 +1025,6 @@ NVME_USE_NVD	opt_nvme.h
 
 # amdsbwd options
 AMDSBWD_DEBUG	opt_amdsbwd.h
+
+# GMEM options
+GMEM 			opt_global.h
diff --git a/sys/kern/subr_bus.c b/sys/kern/subr_bus.c
index e12cb8327b4..da010391eee 100644
--- a/sys/kern/subr_bus.c
+++ b/sys/kern/subr_bus.c
@@ -72,6 +72,8 @@ __FBSDID("$FreeBSD$");
 
 #include <ddb/ddb.h>
 
+#include <sys/gmem.h>
+
 SYSCTL_NODE(_hw, OID_AUTO, bus, CTLFLAG_RW, NULL, NULL);
 SYSCTL_ROOT_NODE(OID_AUTO, dev, CTLFLAG_RW, NULL, NULL);
 
@@ -145,6 +147,8 @@ struct device {
 
 	struct sysctl_ctx_list sysctl_ctx; /**< state for sysctl variables  */
 	struct sysctl_oid *sysctl_tree;	/**< state for sysctl variables */
+
+	gmem_dev_t *gmem_dev; /* gmem device */
 };
 
 static MALLOC_DEFINE(M_BUS, "bus", "Bus data structures");
@@ -3104,6 +3108,17 @@ device_set_unit(device_t dev, int unit)
 	return (0);
 }
 
+void
+device_set_gmem_dev(device_t dev, gmem_dev_t *gmem_dev)
+{
+	dev->gmem_dev = gmem_dev;
+}
+
+gmem_dev_t *
+device_get_gmem_dev(device_t dev)
+{
+	return dev->gmem_dev;
+}
 /*======================================*/
 /*
  * Some useful method implementations to make life easier for bus drivers.
diff --git a/sys/sys/bus.h b/sys/sys/bus.h
index 898f00b50ac..5103b261da3 100644
--- a/sys/sys/bus.h
+++ b/sys/sys/bus.h
@@ -35,6 +35,7 @@
 #include <machine/_bus.h>
 #include <sys/_bus_dma.h>
 #include <sys/ioccom.h>
+#include <sys/gmem.h>
 
 /**
  * @defgroup NEWBUS newbus - a generic framework for managing devices
@@ -636,6 +637,8 @@ int	device_set_unit(device_t dev, int unit);	/* XXX DONT USE XXX */
 int	device_shutdown(device_t dev);
 void	device_unbusy(device_t dev);
 void	device_verbose(device_t dev);
+void device_set_gmem_dev(device_t dev, gmem_dev_t *gmem_dev);
+gmem_dev_t *device_get_gmem_dev(device_t dev);
 
 /*
  * Access functions for devclass.
diff --git a/sys/sys/gmem.h b/sys/sys/gmem.h
new file mode 100644
index 00000000000..9963d127044
--- /dev/null
+++ b/sys/sys/gmem.h
@@ -0,0 +1,34 @@
+/*-
+ * 
+ * This software was developed by Weixi Zhu <wxzhu@rice.edu>
+ * for gmem project.
+ *
+ * GMEM is a generic memory management interface for CPU and devices
+ * It currently only supports x86_64 platforms.
+ *
+ */
+
+#ifndef _GMEM_H_
+#define	_GMEM_H_
+
+// Assumptions: 
+// 	All MMUs share the same physical memory pool
+//  As at current stage we only support EPT and IOMMU
+
+typedef struct gmem_dev gmem_dev_t;
+typedef struct gmem_devmap_info gmem_devmap_t;
+
+typedef struct gmem_mmu_ops gmem_mmu_ops_t;
+typedef struct gmem_uvas gmem_uvas_t;
+typedef struct gmem_uvas_entry gmem_uvas_entry_t;
+typedef struct dev_pmap_replica dev_pmap_replica_t;
+typedef struct dev_pmap dev_pmap_t;
+
+typedef uint8_t gmem_error_t;
+
+// GMEM ERROR Code
+#define GMEM_OK			0x0
+#define GMEM_ENOMEM 	0x1
+#define GMEM_EOVERFLOW 	0x2
+
+#endif
\ No newline at end of file
diff --git a/sys/x86/iommu/busdma_dmar.c b/sys/x86/iommu/busdma_dmar.c
index 341f9cab1f5..375588afcb6 100644
--- a/sys/x86/iommu/busdma_dmar.c
+++ b/sys/x86/iommu/busdma_dmar.c
@@ -68,6 +68,11 @@ __FBSDID("$FreeBSD$");
 #include <x86/iommu/busdma_dmar.h>
 #include <x86/iommu/intel_dmar.h>
 
+#include <sys/gmem.h>
+#include <amd64/gmem/gmem_dev.h>
+#include <amd64/gmem/gmem_uvas.h>
+#include <x86/iommu/intel_iommu.h>
+
 /*
  * busdma_dmar.c, the implementation of the busdma(9) interface using
  * DMAR units from Intel VT-d.
@@ -346,6 +351,12 @@ dmar_bus_dma_tag_create(bus_dma_tag_t parent, bus_size_t alignment,
 	newtag->ctx = oldtag->ctx;
 	newtag->owner = oldtag->owner;
 
+	// GMEM code: inherit parent gmem structures
+	// Note that DMA Tag only restricts DMA transactions, not DMA mappings
+	// So, both dev and uvas must be created upon the context creation time.
+	newtag->gmem_dev = oldtag->gmem_dev;
+
+
 	*dmat = (bus_dma_tag_t)newtag;
 out:
 	CTR4(KTR_BUSDMA, "%s returned tag %p tag flags 0x%x error %d",
@@ -369,7 +380,7 @@ dmar_bus_dma_tag_destroy(bus_dma_tag_t dmat1)
 
 	error = 0;
 	dmat_copy = dmat = (struct bus_dma_tag_dmar *)dmat1;
-
+ 
 	if (dmat != NULL) {
 		if (dmat->map_count != 0) {
 			error = EBUSY;
@@ -384,6 +395,13 @@ dmar_bus_dma_tag_destroy(bus_dma_tag_t dmat1)
 				free_domain(dmat->segments, M_DMAR_DMAMAP);
 				free(dmat, M_DEVBUF);
 				dmat = parent;
+
+				// GMEM code: remove the gmem device here
+				// However, we do not actually remove it, because the parent device
+				// will just hold it.
+				// This will shortcut future redundant gmem_dev_add for childrens of
+				// this parent device.
+				// gmem_dev_remove(dmat->gmem_dev);
 			} else
 				dmat = NULL;
 		}
@@ -867,16 +885,21 @@ dmar_bus_dmamap_sync(bus_dma_tag_t dmat, bus_dmamap_t map,
 }
 
 struct bus_dma_impl bus_dma_dmar_impl = {
+	// GMEM dev involved
 	.tag_create = dmar_bus_dma_tag_create,
 	.tag_destroy = dmar_bus_dma_tag_destroy,
+
 	.tag_set_domain = dmar_bus_dma_tag_set_domain,
 	.map_create = dmar_bus_dmamap_create,
 	.map_destroy = dmar_bus_dmamap_destroy,
 	.mem_alloc = dmar_bus_dmamem_alloc,
 	.mem_free = dmar_bus_dmamem_free,
+
+	// GMEM uvas involved
 	.load_phys = dmar_bus_dmamap_load_phys,
 	.load_buffer = dmar_bus_dmamap_load_buffer,
 	.load_ma = dmar_bus_dmamap_load_ma,
+
 	.map_waitok = dmar_bus_dmamap_waitok,
 	.map_complete = dmar_bus_dmamap_complete,
 	.map_unload = dmar_bus_dmamap_unload,
diff --git a/sys/x86/iommu/busdma_dmar.h b/sys/x86/iommu/busdma_dmar.h
index 063f761dec5..9ced1a2e03e 100644
--- a/sys/x86/iommu/busdma_dmar.h
+++ b/sys/x86/iommu/busdma_dmar.h
@@ -43,6 +43,11 @@ struct bus_dma_tag_dmar {
 	device_t owner;
 	int map_count;
 	bus_dma_segment_t *segments;
+
+	// GMEM code: track gmem_dev in tag_dmar structure
+	// Note that gmem_dev->device is not necessarily the owner of this tag,
+	// but must be the requester device of the IOMMU.
+	gmem_dev_t *gmem_dev;
 };
 
 struct bus_dmamap_dmar {
diff --git a/sys/x86/iommu/intel_ctx.c b/sys/x86/iommu/intel_ctx.c
index 87654c0fedf..9537cd95d82 100644
--- a/sys/x86/iommu/intel_ctx.c
+++ b/sys/x86/iommu/intel_ctx.c
@@ -71,6 +71,11 @@ __FBSDID("$FreeBSD$");
 #include <x86/iommu/intel_dmar.h>
 #include <dev/pci/pcivar.h>
 
+#include <sys/gmem.h>
+#include <amd64/gmem/gmem_dev.h>
+#include <amd64/gmem/gmem_uvas.h>
+#include <x86/iommu/intel_iommu.h>
+
 static MALLOC_DEFINE(M_DMAR_CTX, "dmar_ctx", "Intel DMAR Context");
 static MALLOC_DEFINE(M_DMAR_DOMAIN, "dmar_dom", "Intel DMAR Domain");
 
@@ -476,6 +481,19 @@ dmar_get_ctx_for_dev1(struct dmar_unit *dmar, device_t dev, uint16_t rid,
 		 * higher chance to succeed if the sleep is allowed.
 		 */
 		DMAR_UNLOCK(dmar);
+
+		// GMEM Code: The gmem dev must instantiate a uvas along with
+		// the ctx instantiation.
+		// The dev happens to be the requester which actually uses IOMMU
+		// So, it could be a good idea to check if the device is a gmem_device
+		if (!is_gmem_dev(dev))
+		{
+			// GMEM code: register this gmem device using iommu_ops
+			// Let's not panic, it could be normal
+			// panic("requesting device was not registered as a gmem device\n");
+			gmem_dev_add(dev, &intel_iommu_ops);
+		}
+
 		dmar_ensure_ctx_page(dmar, PCI_RID2BUS(rid));
 		domain1 = dmar_domain_alloc(dmar, id_mapped);
 		if (domain1 == NULL) {
diff --git a/sys/x86/iommu/intel_drv.c b/sys/x86/iommu/intel_drv.c
index 02daa05b0ca..86502280766 100644
--- a/sys/x86/iommu/intel_drv.c
+++ b/sys/x86/iommu/intel_drv.c
@@ -72,6 +72,11 @@ __FBSDID("$FreeBSD$");
 #include <dev/pci/pcivar.h>
 #include <x86/iommu/intel_dmar.h>
 
+#include <sys/gmem.h>
+#include <amd64/gmem/gmem_dev.h>
+#include <amd64/gmem/gmem_uvas.h>
+#include <x86/iommu/intel_iommu.h>
+
 #ifdef DEV_APIC
 #include "pcib_if.h"
 #include <machine/intr_machdep.h>
@@ -168,6 +173,7 @@ dmar_identify(driver_t *driver, device_t parent)
 	ACPI_DMAR_HARDWARE_UNIT *dmarh;
 	ACPI_STATUS status;
 	int i, error;
+	gmem_dev_t *gmem_dev_tmp;
 
 	if (acpi_disabled("dmar"))
 		return;
@@ -208,15 +214,22 @@ dmar_identify(driver_t *driver, device_t parent)
 			printf("dmar_identify: cannot create instance %d\n", i);
 			continue;
 		}
+
 		error = bus_set_resource(dmar_devs[i], SYS_RES_MEMORY,
 		    DMAR_REG_RID, dmarh->Address, PAGE_SIZE);
 		if (error != 0) {
 			printf(
 	"dmar%d: unable to alloc register window at 0x%08jx: error %d\n",
 			    i, (uintmax_t)dmarh->Address, error);
+
 			device_delete_child(parent, dmar_devs[i]);
 			dmar_devs[i] = NULL;
 		}
+
+		// GMEM code: register this gmem device using iommu_ops
+		gmem_dev_tmp = gmem_dev_add(dmar_devs[i], &intel_iommu_ops);
+		// GMEM code: revert the registration of the gmem_dev upon errors
+		// gmem_dev_remove(gmem_dev_tmp);
 	}
 }
 
diff --git a/sys/x86/iommu/intel_gas.c b/sys/x86/iommu/intel_gas.c
index cdb462c2a05..9ab557c486b 100644
--- a/sys/x86/iommu/intel_gas.c
+++ b/sys/x86/iommu/intel_gas.c
@@ -70,6 +70,11 @@ __FBSDID("$FreeBSD$");
 #include <dev/pci/pcireg.h>
 #include <x86/iommu/intel_dmar.h>
 
+#include <sys/gmem.h>
+#include <amd64/gmem/gmem_dev.h>
+#include <amd64/gmem/gmem_uvas.h>
+#include <x86/iommu/intel_iommu.h>
+
 /*
  * Guest Address Space management.
  */
diff --git a/sys/x86/iommu/intel_idpgtbl.c b/sys/x86/iommu/intel_idpgtbl.c
index 8103b49b3ad..cdad022fa83 100644
--- a/sys/x86/iommu/intel_idpgtbl.c
+++ b/sys/x86/iommu/intel_idpgtbl.c
@@ -69,6 +69,11 @@ __FBSDID("$FreeBSD$");
 #include <dev/pci/pcireg.h>
 #include <x86/iommu/intel_dmar.h>
 
+#include <sys/gmem.h>
+#include <amd64/gmem/gmem_dev.h>
+#include <amd64/gmem/gmem_uvas.h>
+#include <x86/iommu/intel_iommu.h>
+
 static int domain_unmap_buf_locked(struct dmar_domain *domain,
     dmar_gaddr_t base, dmar_gaddr_t size, int flags);
 
diff --git a/sys/x86/iommu/intel_iommu.c b/sys/x86/iommu/intel_iommu.c
new file mode 100644
index 00000000000..e77dfb4b0d7
--- /dev/null
+++ b/sys/x86/iommu/intel_iommu.c
@@ -0,0 +1,154 @@
+/*-
+ * 
+ * This software was developed by Weixi Zhu <wxzhu@rice.edu>
+ * for gmem project.
+ *
+ * GMEM is a generic memory management interface for CPU and devices
+ * It currently only supports x86_64 platforms.
+ *
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/malloc.h>
+#include <sys/bus.h>
+#include <sys/interrupt.h>
+#include <sys/kernel.h>
+#include <sys/ktr.h>
+#include <sys/lock.h>
+#include <sys/memdesc.h>
+#include <sys/mutex.h>
+#include <sys/proc.h>
+#include <sys/rwlock.h>
+#include <sys/rman.h>
+#include <sys/sf_buf.h>
+#include <sys/sysctl.h>
+#include <sys/taskqueue.h>
+#include <sys/tree.h>
+#include <sys/uio.h>
+#include <sys/vmem.h>
+#include <vm/vm.h>
+#include <vm/vm_extern.h>
+#include <vm/vm_kern.h>
+#include <vm/vm_object.h>
+#include <vm/vm_page.h>
+#include <vm/vm_pager.h>
+#include <vm/vm_map.h>
+#include <machine/atomic.h>
+#include <machine/bus.h>
+#include <machine/cpu.h>
+#include <machine/md_var.h>
+#include <machine/specialreg.h>
+#include <x86/include/busdma_impl.h>
+#include <x86/iommu/intel_reg.h>
+#include <x86/iommu/busdma_dmar.h>
+#include <dev/pci/pcireg.h>
+#include <x86/iommu/intel_dmar.h>
+
+#include <sys/gmem.h>
+#include <amd64/gmem/gmem_dev.h>
+#include <amd64/gmem/gmem_uvas.h>
+#include <x86/iommu/intel_iommu.h>
+
+static gmem_error_t intel_iommu_pmap_create(dev_pmap_t *pmap, void *dev_data)
+{
+	intel_iommu_pgtable_t *pgtable;
+	vm_page_t m;
+
+	KASSERT(pmap->data == NULL, "creating a pmap over existing page table");
+	pmap->data = malloc(sizeof(intel_iommu_pgtable_t), M_DEVBUF, M_WAITOK | M_ZERO);
+
+
+	pgtable = pmap->data;
+	// TODO: equivalent semantic conversion first
+	pgtable->pglvl = 4;
+	pgtable->id_mapped = ((intel_iommu_pgtable_t *) dev_data)->id_mapped;
+	pgtable->dmar = ((intel_iommu_pgtable_t *) dev_data)->dmar;
+	pgtable->domain = ((intel_iommu_pgtable_t *) dev_data)->domain;
+	pgtable->pgtbl_obj = vm_pager_allocate(OBJT_PHYS, NULL,
+	    IDX_TO_OFF(pglvl_max_pages(pgtable->pglvl)), 0, 0, NULL);
+	
+	VM_OBJECT_WLOCK(pgtable->pgtbl_obj);
+	m = dmar_pgalloc(pgtable->domain->pgtbl_obj, 0, DMAR_PGF_WAITOK |
+	    DMAR_PGF_ZERO | DMAR_PGF_OBJL);
+	/* No implicit free of the top level page table page. */
+	m->wire_count = 1;
+	VM_OBJECT_WUNLOCK(pgtable->pgtbl_obj);
+
+	// DMAR_LOCK(domain->dmar);
+	// domain->flags |= DMAR_DOMAIN_PGTBL_INITED;
+	// DMAR_UNLOCK(domain->dmar);
+	return GMEM_OK;
+}
+
+static gmem_error_t intel_iommu_pmap_destroy(dev_pmap_t *pmap)
+{
+	intel_iommu_pgtable_t *pgtable;
+	vm_object_t obj;
+	struct dmar_unit *dmar;
+	struct dmar_domain *domain;
+	vm_page_t m;
+
+	pgtable = (intel_iommu_pgtable_t *) pmap->data;
+	obj = pgtable->pgtbl_obj;
+	dmar = pgtable->dmar;
+	domain = pgtable->domain;
+
+	if (obj == NULL) {
+		KASSERT((domain->dmar->hw_ecap & DMAR_ECAP_PT) != 0 &&
+		    (domain->flags & DMAR_DOMAIN_IDMAP) != 0,
+		    ("lost pagetable object domain %p", domain));
+		// return;
+	}
+	else
+	{
+		// We save all changes in the future after the conversion
+		// Don't call this function with a lock state issued outside
+		DMAR_DOMAIN_ASSERT_PGLOCKED(domain);
+		// VM_OBJECT_WLOCK(obj);
+		domain->pgtbl_obj = NULL;
+
+		if ((domain->flags & DMAR_DOMAIN_IDMAP) != 0) {
+			put_idmap_pgtbl(obj);
+			domain->flags &= ~DMAR_DOMAIN_IDMAP;
+			return GMEM_OK;
+		}
+
+		/* Obliterate wire_counts */
+		for (m = vm_page_lookup(obj, 0); m != NULL; m = vm_page_next(m))
+			m->wire_count = 0;
+		VM_OBJECT_WUNLOCK(obj);
+		vm_object_deallocate(obj);
+	}
+	return GMEM_OK;
+}
+
+static gmem_error_t intel_iommu_pmap_enter(vm_offset_t va, vm_size_t size, 
+	vm_paddr_t pa, vm_prot_t protection)
+{
+	return GMEM_OK;
+}
+
+static gmem_error_t intel_iommu_pmap_release(vm_offset_t va, vm_size_t size)
+{
+	return GMEM_OK;
+}
+
+static gmem_error_t intel_iommu_pmap_protect(vm_offset_t va, vm_size_t size,
+	vm_prot_t new_prot)
+{
+	return GMEM_OK;
+}
+
+gmem_mmu_ops_t intel_iommu_ops = {
+	.pgsize_bitmap = (1UL << 12) | (1UL << 21) | (1UL << 30),
+	.mmu_has_range_tlb = false,
+	.mmu_pmap_create = intel_iommu_pmap_create,
+	.mmu_pmap_destroy = intel_iommu_pmap_destroy,
+	.mmu_pmap_enter = intel_iommu_pmap_enter,
+	.mmu_pmap_release = intel_iommu_pmap_release,
+	.mmu_pmap_protect = intel_iommu_pmap_protect,
+};
\ No newline at end of file
diff --git a/sys/x86/iommu/intel_iommu.h b/sys/x86/iommu/intel_iommu.h
new file mode 100644
index 00000000000..e5bba596799
--- /dev/null
+++ b/sys/x86/iommu/intel_iommu.h
@@ -0,0 +1,54 @@
+/*-
+ * 
+ * This software was developed by Weixi Zhu <wxzhu@rice.edu>
+ * for gmem project.
+ *
+ * GMEM is a generic memory management interface for CPU and devices
+ * It currently only supports x86_64 platforms.
+ *
+ */
+
+#ifndef _INTEL_IOMMU_H_
+#define	_INTEL_IOMMU_H_
+
+struct intel_iommu_dev_data
+{
+	bool id_mapped;
+	struct dmar_unit *dmar;
+	struct dmar_domain *domain;
+};
+
+struct intel_iommu_pgtable
+{
+	// some junk from dmar_domain, delete all the followings in the future
+	int mgaw;			/* (c) Real max address width */
+	int agaw;			/* (c) Adjusted guest address width */
+	int pglvl;			/* (c) The pagelevel */
+	int awlvl;			/* (c) The pagelevel as the bitmask,
+					   to set in context entry */
+	u_int batch_no;
+
+	bool id_mapped;
+
+	// hardware information, iommus could differ, so it is required
+	struct dmar_unit *dmar;
+
+	// process context information, delete if not required
+	struct dmar_domain *domain;
+
+	// real page table data
+	vm_object_t pgtbl_obj;
+
+	// The original iommu data structure is messy -- 
+	//	domain contains both page table and address space data
+	//  dmar contains async queues
+	//  domain members use dmar lock???
+};
+
+typedef struct intel_iommu_dev_data intel_iommu_dev_data_t;
+typedef struct intel_iommu_pgtable intel_iommu_pgtable_t;
+
+// both identity mapping and normal mappings should be supported
+extern gmem_mmu_ops_t intel_iommu_ops;
+
+#endif
\ No newline at end of file
